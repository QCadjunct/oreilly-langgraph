{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "# Here's a simplified version of a RAG (Retrieval-Augmented Generation) agent that includes only a simple retriever tool along with the start and end nodes. This version will demonstrate the basic agent loop concepts without the grading and other complexities.\n",
    "\n",
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# Simplified RAG Agent\n",
    "\n",
    "from langchain import hub\n",
    "from langchain_core.messages import BaseMessage, HumanMessage\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.graph import MessagesState, StateGraph, START, END\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "import os\n",
    "import getpass\n",
    "\n",
    "# Set environment keys\n",
    "def _set_env(key: str):\n",
    "    if key not in os.environ:\n",
    "        os.environ[key] = getpass.getpass(f\"{key}:\")\n",
    "\n",
    "_set_env(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Load documents\n",
    "urls_from_pj = [\n",
    "    \"https://blog.langchain.dev/launching-long-term-memory-support-in-langgraph/\",\n",
    "    \"https://kavourei.github.io/LonTermMemory\",\n",
    "    \"https://github.com/langchain-ai/langgra\"\n",
    "]\n",
    "docs = [WebBaseLoader(url).load() for url in urls_from_pj]\n",
    "docs_list = [item for sublist in docs for item in sublist]\n",
    "\n",
    "# Split documents\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=100, chunk_overlap=50\n",
    ")\n",
    "doc_splits = text_splitter.split_documents(docs_list)\n",
    "\n",
    "# Add to vectorDB\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=doc_splits,\n",
    "    collection_name=\"simple-rag-chroma\",\n",
    "    embedding=OpenAIEmbeddings(),\n",
    ")\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What does Lilian Weng say about the types of agent memory?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  retrieve_blog_post (call_9wFMcOQDrHo9XvDkOyKu607G)\n",
      " Call ID: call_9wFMcOQDrHo9XvDkOyKu607G\n",
      "  Args:\n",
      "    query: types of agent memory\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: retrieve_blog_post\n",
      "\n",
      "we've realized something important: there's no universally perfect solution for AI memory. The best memory for each application still contains very application specific logic. By extension, most \"agent memory\" products today are too high-level. They try to create a one-size-fits-all product that doesn't satisfy many production users' needs.This insight is why we have built our initial memory support into LangGraph as a simple document store. High level abstractions can be easily built on top (as\n",
      "\n",
      "user preferences. This feature is part of the OSS library, and it is enabled by default for all LangGraph Cloud & Studio users.Memory: from short (thread-scoped) to long (cross-thread)On MemoryMost AI applications today are goldfish; they forget everything between conversations. This isn't just inefficient— it fundamentally limits what AI can do.Over the past year at LangChain, we've been working with customers to build memory into their agents. Through this experience,\n",
      "\n",
      "An end-to-end tutorial video that guides users through the implementation process.\n",
      "A LangGraph Memory Agent example in Python.\n",
      "A LangGraph.js Memory Agent example in JavaScript.\n",
      "\n",
      "today are goldfish; they forget everything between conversations. This isn't just inefficient— it fundamentally limits what AI can do.Over the past year at LangChain, we've been working with customers to build memory into their agents. Through this experience, we've realized something important: there's no universally perfect solution for AI memory. The best memory for each application still contains very application specific logic. By extension, most \"agent memory\" products today are too high-level. They try to create a\n"
     ]
    }
   ],
   "source": [
    "# Create retriever tool\n",
    "retriever_tool = create_retriever_tool(\n",
    "    retriever,\n",
    "    \"retrieve_blog_post\",\n",
    "    \"Retrieve information about Lilian Weng blog posts on agents.\",\n",
    ")\n",
    "\n",
    "tools = [retriever_tool]\n",
    "\n",
    "# Agent function\n",
    "def agent(state: MessagesState) -> MessagesState:\n",
    "    \"\"\"Invoke the agent to decide whether to retrieve.\"\"\"\n",
    "    model = ChatOpenAI(temperature=0, streaming=True, model=\"gpt-4o-mini\")\n",
    "    model = model.bind_tools(tools)\n",
    "    response = model.invoke(state[\"messages\"])\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "# Define a simple graph with only start and end nodes\n",
    "workflow = StateGraph(MessagesState)\n",
    "workflow.add_node(\"agent\", agent)\n",
    "retrieve = ToolNode([retriever_tool])\n",
    "workflow.add_node(\"retrieve\", retrieve)\n",
    "workflow.add_edge(START, \"agent\")\n",
    "workflow.add_edge(\"agent\", \"retrieve\")\n",
    "workflow.add_edge(\"retrieve\", END)\n",
    "\n",
    "# Compile the graph\n",
    "graph = workflow.compile()\n",
    "\n",
    "# Input message\n",
    "inputs = {\n",
    "    \"messages\": [\n",
    "        (\"user\", \"What does Lilian Weng say about the types of agent memory?\"),\n",
    "    ]\n",
    "}\n",
    "\n",
    "output = graph.invoke(inputs)\n",
    "\n",
    "for m in output[\"messages\"]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oreilly-langgraph",
   "language": "python",
   "name": "oreilly-langgraph"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
